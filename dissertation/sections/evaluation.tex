\chapter{Evaluation}

The project can be considered to have been successful, with all of the primary and secondary objectives achieved. Additional requirements, not included in the objectives, were also achieved. The project has evolved quickly and future work is already planned as discussed in Chapter ~\ref{chap:futureWork}. 

\section{The Hardware}

Designing and implementing the two HaptiQs has been a very challenging task. While the project is heavily inspired by the HTP \cite{marquardt2009haptic}, the device created in this project has been completely reinvented. I found the process of imagining 3D objects stimulating. But, the low precision of the 3D printer transformed the process into a demanding one. This was particularly true for the first device, the 4-HaptiQ. 

The miniaturization process, with the 8-HaptiQ, added additional challenges. Assembling the second device, for instance, requires more time and precision. Some pieces, therefore, are not as easy to assemble as wanted. Nonetheless, being the HaptiQ only a prototype the focus has been more on creating a proof of concept, rather than a final product. 

No user study was conducted during this project. However, Saad, being visually impaired (he is one of the two collaborators, see Section ~\ref{sec:collaborators}), has provided some informal feedback on the first version of the device. He used a not-fully working version of the 4-HaptiQ, with no pressure sensors, a Bytetag to locate the position of the device on the interactive table, and point-reference actuator. The first note that he made was about the point-reference actuator, which was disrupting few times. I also observed that even if the device did not work properly, he could still follow object edges.  

I was unable to organize any meetings with Saad to get some feedback about the 8-HaptiQ. However, we plan to meet sometime in the near future to start working on the next iteration phase (see Chapter ~\ref{chap:futureWork}). 

\section{The API}

The structure of the API has changed few times, based on how the hardware evolved, the challenged faced and the new requirements that were introduced during the project. 
Initially, the API was designed to support input location via Bytetags only. However, glyphs had to be introduced to increase reliability and accuracy. In the month of February I was also told that Régis (he is one of the two collaborators, see Section ~\ref{sec:collaborators}) would join the project in March and that he will then use an interactive table not supporting the Microsoft Surface SDK 2.0. Therefore, I removed any dependency from the Surface SDK from the HaptiQ API and created a separate API to retrieve the input locations of the devices. 

The API has been designed and implemented to allow clients to easily use and possibly extend it. Régis has been using the API since the \nth{18} March 2014, without any major difficulties. He also played the role of the client in the project, by submitting issues about the project through GitHub. Some issues are still not solved, because not relevant to the objectives of this project. Other issues are simply not solvable due to technology limitations. For instance, at the moment it is not possible to define when a device is currently on the table or not. This is due to some inaccuracy issues of the GRATF library in recognising the glyphs and/or on the table providing raw images with low quality. A possible solution could be to classify a device as not being on the table whenever a glyph is not being recognised for more than a certain time threshold. 

\todo[inline, color=green!40]{can use hidden-markov-model or time-series-analysis to identify patterns in pressure input and classify gestures (also add graph of pressure input, bootcamp hd)}

\section{Tactons}

To the best of my knowledge, this work is the first to explore dynamic vector-based tactons to exploit haptic cues of edges, corners and directions. The tactons proposed by Pietrzak et al. \cite{pietrzak2009creating} are based on pin arrays. In his paper, it is interesting to note how the designed tactons evolved as user experiments were conducted. The tactons used for the HaptiQ, instead, are merely based on common sense rather than experimentation. 

During one of the meetings with Saad, I discussed with him a subset of the tactons presented in this report. His feedback was positive. The meeting after, when trying the HaptiQ, he was able to decode the behaviours efficiently. This cannot be considered a study of any kind, but his feedback was very valuable to define how the HaptiQ has evolved so far. 

\section{Applications}

\todo[inline, color=green!40]{discuss applications}

\section{Summary}

This project was very open-ended and consisted of many different components. Initially, I planned to conduct a user study to evaluate the HaptiQ. However, due to major ethical issues, I decided, with my supervisor, to focus more on the engineering aspects of the HaptiQ and its API. However, being this a research project there were never well defined requirements. Instead, an iterative process was used. This allowed me to created two devices, with the second one completely re-engineered.   

As I previously stated, no user study has been conducted to formally evaluate the device. However, while a user study is already planned in the near future (see Chapter ~\ref{chap:futureWork}), some conclusions can be drawn by comparing the device with the state of the art in haptic technology. 

\todo[inline, color=green!40]{compare to current state of the art}

The tertiary objectives included: enabling the haptic device to be used collaboratively and allowing the device to sense textures. 
With the focus of the project being on the haptic cues that the HaptiQ could enhance, I decided to de-prioritise any possible feature that could allow communication between two devices. However, the API can support multiple devices. Thus, clients can implement  collaborative features on top of the API.
On the other hand, the objective of adding the possibility to sense texture was not attempted because the frequency at which the servos, used for this project, work is too low \cite{brown2005first}. A possible alternative could be to add a central static actuator to the devices, similar to the point-reference actuator, which could convey texture information to users via electro-vibration \cite{bau2010teslatouch, bau2012revel} or mechanical vibrations \footnote{http://www.precisionmicrodrives.com/vibrating-vibrator-vibration-motors}.   